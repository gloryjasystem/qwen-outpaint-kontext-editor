{
  "input": {
    "workflow": {
      "1": {
        "inputs": {
          "ckpt_name": "model.safetensors"
        },
        "class_type": "CheckpointLoaderSimple",
        "_meta": {
          "title": "Load Checkpoint"
        }
      },
      "2": {
        "inputs": {
          "stop_at_clip_layer": -2,
          "clip": [
            "1",
            1
          ]
        },
        "class_type": "CLIPSetLastLayer",
        "_meta": {
          "title": "CLIP Set Last Layer"
        }
      },
      "3": {
        "inputs": {
          "text": "wearing red modern dress",
          "clip": [
            "15",
            1
          ]
        },
        "class_type": "CLIPTextEncode",
        "_meta": {
          "title": "CLIP Text Encode (Prompt)"
        }
      },
      "4": {
        "inputs": {
          "text": "easynegative,(deformed, distorted, disfigured:1.3), poorly drawn, bad anatomy, wrong anatomy, extra limb, missing limb, floating limbs, (mutated hands and fingers:1.4), disconnected limbs, mutation, mutated, ugly, disgusting, blurry, amputation",
          "clip": [
            "15",
            1
          ]
        },
        "class_type": "CLIPTextEncode",
        "_meta": {
          "title": "CLIP Text Encode (Prompt)"
        }
      },
      "5": {
        "inputs": {
          "seed": 308738993914180,
          "steps": 20,
          "cfg": 7,
          "sampler_name": "dpmpp_2m",
          "scheduler": "karras",
          "denoise": 1,
          "model": [
            "15",
            0
          ],
          "positive": [
            "9",
            0
          ],
          "negative": [
            "9",
            1
          ],
          "latent_image": [
            "9",
            2
          ]
        },
        "class_type": "KSampler",
        "_meta": {
          "title": "KSampler"
        }
      },
      "7": {
        "inputs": {
          "samples": [
            "5",
            0
          ],
          "vae": [
            "1",
            2
          ]
        },
        "class_type": "VAEDecode",
        "_meta": {
          "title": "VAE Decode"
        }
      },
      "8": {
        "inputs": {
          "filename_prefix": "output",
          "images": [
            "12",
            0
          ]
        },
        "class_type": "SaveImage",
        "_meta": {
          "title": "Save Image"
        }
      },
      "9": {
        "inputs": {
          "noise_mask": true,
          "positive": [
            "3",
            0
          ],
          "negative": [
            "4",
            0
          ],
          "vae": [
            "1",
            2
          ],
          "pixels": [
            "11",
            1
          ],
          "mask": [
            "11",
            2
          ]
        },
        "class_type": "InpaintModelConditioning",
        "_meta": {
          "title": "InpaintModelConditioning"
        }
      },
      "10": {
        "inputs": {
          "image": "input_image.png",
          "upload": "image"
        },
        "class_type": "LoadImage",
        "_meta": {
          "title": "Load Input Image"
        }
      },
      "11": {
        "inputs": {
          "context_expand_pixels": 20,
          "context_expand_factor": 1,
          "fill_mask_holes": true,
          "blur_mask_pixels": 16,
          "invert_mask": false,
          "blend_pixels": 16,
          "rescale_algorithm": "bicubic",
          "mode": "forced size",
          "force_width": 1024,
          "force_height": 1024,
          "rescale_factor": 1,
          "min_width": 512,
          "min_height": 512,
          "max_width": 768,
          "max_height": 768,
          "padding": 32,
          "image": [
            "10",
            0
          ],
          "mask": [
            "17",
            1
          ]
        },
        "class_type": "InpaintCrop",
        "_meta": {
          "title": "Inpaint Crop"
        }
      },
      "12": {
        "inputs": {
          "rescale_algorithm": "bislerp",
          "stitch": [
            "11",
            0
          ],
          "inpainted_image": [
            "7",
            0
          ]
        },
        "class_type": "InpaintStitch",
        "_meta": {
          "title": "Inpaint Stitch"
        }
      },
      "15": {
        "inputs": {
          "lora_name": "lady.safetensors",
          "strength_model": 1.0000000000000002,
          "strength_clip": 1.0000000000000002,
          "model": [
            "1",
            0
          ],
          "clip": [
            "2",
            0
          ]
        },
        "class_type": "LoraLoader",
        "_meta": {
          "title": "Load LoRA"
        }
      },

      "17": {
        "inputs": {
          "image": "mask_image.png",
          "upload": "image"
        },
        "class_type": "LoadImage",
        "_meta": {
          "title": "Load Mask Image"
        }
      }
    },
    "images": [
      {
        "name": "input_image.png",
        "image": ""
      },
      {
        "name": "mask_image.png", 
        "image": ""
      }
    ]
  }
}